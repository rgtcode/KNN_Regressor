# -*- coding: utf-8 -*-
"""KNN_Regressor_California_Housing_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10se73rViX3nTQ5oMLVRcnuKA-0hRMBtx

## Importing  libraries
"""

# Scikit-Learn â‰¥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error 
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

"""##Loading California Housing dataset"""

# Download the data
X,y = fetch_california_housing(return_X_y=True)

"""## Split data into train and test sets"""

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=8)

"""##Preprocessing the dataset

Making the bargraphs to understand what kind of preprocessing needed to be done
"""

california_housing=fetch_california_housing(as_frame=True)
california_housing.frame.hist(figsize=(12,10),bins=30,edgecolor="black")
plt.subplots_adjust(hspace=0.7,wspace=0.4)

"""From the above distribution we can see that we need to apply the following preprocessing 

1.   **Feature Scaling**-Feature scaling is an essential step in algorithms like KNN because here we are dealing with metrics like euclidian distance which are dependent on the scale of the dataset.So for scaling we will use the StandardScaler function in sklearn




"""

# Create pipeline with min-max scaler followed by 
# KNN regressor
pipe = Pipeline([('scaler', MinMaxScaler()), 
                 ('knn', KNeighborsRegressor(n_neighbors=2))])

#fitting and transform training data
pipe.fit(X_train,y_train)

#transform test data
y_pred = pipe.predict(X_test) 

# compute RMSE
error = mean_squared_error(y_test,y_pred, squared=False)
print(error)

"""

### Finding the best value of k through cross validation
In case of KNN the no. of neighbours is the hyperparameter that we have to tune it through cross validation"""

#to store rmse values for different k
rmse_val = [] 

for K in range(1,31):
    
  pipe = Pipeline([('scaler', MinMaxScaler()), 
                   ('knn', KNeighborsRegressor(n_neighbors=K))])
  
  #fit the model
  pipe.fit(X_train, y_train)  

  # make prediction on test set
  pred=pipe.predict(X_test) 
  
  # calculate rmse
  error = mean_squared_error(y_test,pred, squared=False)
  
  #store rmse values
  rmse_val.append(error)

"""## Learning Curve

### Plotting RMSE vs K, curve to find best value of K
"""

plt.figure(figsize=(6,6))

#plotting the rmse values against k values
plt.plot(range(1, len(rmse_val)+1), rmse_val, color='green') 
plt.xlabel('Different values of K', fontsize=20) 
plt.ylabel('RMSE', fontsize=20, rotation=0) 
plt.grid(True)
  
# displaying the title
plt.title("Validations Loss vs K", fontsize=24)
  
plt.show()

# index=np.argmin(rmse_val)
print('Lowest rmse value comes when K is:{}'.format(np.argmin(rmse_val)+1))

"""# Performing Grid Search CV"""

k_range = list(range(1, 31))
params = dict(n_neighbors=k_range)
print(params)

reg_knn = KNeighborsRegressor()

#validate model with his parameters
gs = GridSearchCV(estimator=reg_knn, 
                  param_grid=params, 
                  cv=10, n_jobs=-1) 
gs.fit(X_train, y_train)

reg_knn = gs.best_estimator_
print(reg_knn) #printing best estimator values

model = KNeighborsRegressor(n_neighbors=9)

model.fit(X_train, y_train)
pred=model.predict(X_test) #make prediction on test set
error = mean_squared_error(y_test,pred, squared=False)
print('RMSE value for k is: ' , error)

"""# Performing Randomized search CV"""

reg_knn = KNeighborsRegressor()
param_grid = params
randSearchCV = RandomizedSearchCV(reg_knn, param_grid, cv=10)
randSearchCV.fit(X_train, y_train)

best_reg_knn = randSearchCV.best_estimator_

#best estimator using radomized search cv
print(best_reg_knn)

model = KNeighborsRegressor(n_neighbors=8)

model.fit(X_train, y_train)
pred=model.predict(X_test) #make prediction on test set
error = mean_squared_error(y_test,pred, squared=False)
print(f'RMSE value for k = {8} is: ' , error)

"""# Polynomial Features
poly_features = PolynomialFeatures(degree=2, include_bias=False)
"""

params = {'poly__degree':list(range(1,4)),
          'knn__n_neighbors': list(range(6, 12))}
print(params)

pipe = Pipeline(steps=[('poly', PolynomialFeatures()),
                       ('scaler', MinMaxScaler()),
                       ('knn', KNeighborsRegressor())])

#validate model with his parameters
gs = GridSearchCV(estimator=pipe, 
                  param_grid=params, 
                  cv=5, n_jobs=-1)

gs.fit(X_train, y_train)

reg_knn = gs.best_estimator_

# printing best estimator values
print(reg_knn)